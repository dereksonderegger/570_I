<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Regression | Introduction to Statistical Methodology</title>
  <meta name="description" content="Chapter 10 Regression | Introduction to Statistical Methodology" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Regression | Introduction to Statistical Methodology" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="dereksonderegger/570_I" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Regression | Introduction to Statistical Methodology" />
  
  
  

<meta name="author" content="Derek L. Sonderegger" />


<meta name="date" content="2020-07-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="9-analysis-of-variance-anova.html"/>
<link rel="next" href="11-resampling-linear-models.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://dereksonderegger.github.io/570_I/Statistical-Methods-I.pdf" target="blank">PDF version</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html"><i class="fa fa-check"></i><b>1</b> Summary Statistics and Graphing</a><ul>
<li class="chapter" data-level="1.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#graphical-summaries-of-data"><i class="fa fa-check"></i><b>1.1</b> Graphical summaries of data</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#univariate---categorical"><i class="fa fa-check"></i><b>1.1.1</b> Univariate - Categorical</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#univariate---continuous"><i class="fa fa-check"></i><b>1.1.2</b> Univariate - Continuous</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#bivariate---categorical-vs-continuous"><i class="fa fa-check"></i><b>1.1.3</b> Bivariate - Categorical vs Continuous</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#bivariate---continuous-vs-continuous"><i class="fa fa-check"></i><b>1.1.4</b> Bivariate - Continuous vs Continuous</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#measures-of-centrality"><i class="fa fa-check"></i><b>1.2</b> Measures of Centrality</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#mean"><i class="fa fa-check"></i><b>1.2.1</b> Mean</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#median"><i class="fa fa-check"></i><b>1.2.2</b> Median</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#mode"><i class="fa fa-check"></i><b>1.2.3</b> Mode</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#examples"><i class="fa fa-check"></i><b>1.2.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#measures-of-spread"><i class="fa fa-check"></i><b>1.3</b> Measures of Spread</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#range"><i class="fa fa-check"></i><b>1.3.1</b> Range</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#inter-quartile-range"><i class="fa fa-check"></i><b>1.3.2</b> Inter-Quartile Range</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#variance"><i class="fa fa-check"></i><b>1.3.3</b> Variance</a></li>
<li class="chapter" data-level="1.3.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#standard-deviation"><i class="fa fa-check"></i><b>1.3.4</b> Standard Deviation</a></li>
<li class="chapter" data-level="1.3.5" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#coefficient-of-variation"><i class="fa fa-check"></i><b>1.3.5</b> Coefficient of Variation</a></li>
<li class="chapter" data-level="1.3.6" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#empirical-rule-of-thumb"><i class="fa fa-check"></i><b>1.3.6</b> Empirical Rule of Thumb</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#shape"><i class="fa fa-check"></i><b>1.4</b> Shape</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#symmetry"><i class="fa fa-check"></i><b>1.4.1</b> Symmetry</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#unimodal-or-multi-modal"><i class="fa fa-check"></i><b>1.4.2</b> Unimodal or Multi-modal</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#skew"><i class="fa fa-check"></i><b>1.4.3</b> Skew</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-probability.html"><a href="2-probability.html"><i class="fa fa-check"></i><b>2</b> Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="2-probability.html"><a href="2-probability.html#introduction-to-set-theory"><i class="fa fa-check"></i><b>2.1</b> Introduction to Set Theory</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-probability.html"><a href="2-probability.html#composition-of-events"><i class="fa fa-check"></i><b>2.1.1</b> Composition of events</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-probability.html"><a href="2-probability.html#probability-rules"><i class="fa fa-check"></i><b>2.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-probability.html"><a href="2-probability.html#simple-rules"><i class="fa fa-check"></i><b>2.2.1</b> Simple Rules</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-probability.html"><a href="2-probability.html#conditional-probability"><i class="fa fa-check"></i><b>2.2.2</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-probability.html"><a href="2-probability.html#summary-of-probability-rules"><i class="fa fa-check"></i><b>2.2.3</b> Summary of Probability Rules</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-probability.html"><a href="2-probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>2.3</b> Discrete Random Variables</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-probability.html"><a href="2-probability.html#introduction-to-discrete-random-variables"><i class="fa fa-check"></i><b>2.3.1</b> Introduction to Discrete Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-probability.html"><a href="2-probability.html#common-discrete-distributions"><i class="fa fa-check"></i><b>2.4</b> Common Discrete Distributions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-probability.html"><a href="2-probability.html#binomial-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Binomial Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-probability.html"><a href="2-probability.html#poisson-distribution"><i class="fa fa-check"></i><b>2.4.2</b> Poisson Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-probability.html"><a href="2-probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>2.5</b> Continuous Random Variables</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-probability.html"><a href="2-probability.html#uniform01-distribution"><i class="fa fa-check"></i><b>2.5.1</b> Uniform(0,1) Distribution</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-probability.html"><a href="2-probability.html#exponential-distribution"><i class="fa fa-check"></i><b>2.5.2</b> Exponential Distribution</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-probability.html"><a href="2-probability.html#normal-distribution"><i class="fa fa-check"></i><b>2.5.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="2.5.4" data-path="2-probability.html"><a href="2-probability.html#standardizing"><i class="fa fa-check"></i><b>2.5.4</b> Standardizing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html"><i class="fa fa-check"></i><b>3</b> Confidence Intervals via Bootstrapping</a><ul>
<li class="chapter" data-level="3.1" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#theory-of-bootstrapping"><i class="fa fa-check"></i><b>3.1</b> Theory of Bootstrapping</a></li>
<li class="chapter" data-level="3.2" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#quantile-based-confidence-intervals"><i class="fa fa-check"></i><b>3.2</b> Quantile-based Confidence Intervals</a></li>
<li class="chapter" data-level="3.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>3.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html"><i class="fa fa-check"></i><b>4</b> Sampling Distribution of <span class="math inline">\(\bar{X}\)</span></a><ul>
<li class="chapter" data-level="4.1" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#enlightening-example"><i class="fa fa-check"></i><b>4.1</b> Enlightening Example</a></li>
<li class="chapter" data-level="4.2" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#mathematical-details"><i class="fa fa-check"></i><b>4.2</b> Mathematical details</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#probability-rules-for-expectations-and-variances"><i class="fa fa-check"></i><b>4.2.1</b> Probability Rules for Expectations and Variances</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i><b>4.2.2</b> Mean and Variance of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#distribution-of-barx"><i class="fa fa-check"></i><b>4.3</b> Distribution of <span class="math inline">\(\bar{X}\)</span></a></li>
<li class="chapter" data-level="4.4" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#central-limit-theorem"><i class="fa fa-check"></i><b>4.4</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="4.5" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html"><i class="fa fa-check"></i><b>5</b> Confidence Intervals for <span class="math inline">\(\mu\)</span></a><ul>
<li class="chapter" data-level="5.1" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#asymptotic-result-sigma-known"><i class="fa fa-check"></i><b>5.1</b> Asymptotic result (<span class="math inline">\(\sigma\)</span> known)</a></li>
<li class="chapter" data-level="5.2" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#asymptotoic-result-sigma-unknown"><i class="fa fa-check"></i><b>5.2</b> Asymptotoic result (<span class="math inline">\(\sigma\)</span> unknown)</a></li>
<li class="chapter" data-level="5.3" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#sample-size-selection"><i class="fa fa-check"></i><b>5.3</b> Sample Size Selection</a></li>
<li class="chapter" data-level="5.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html"><i class="fa fa-check"></i><b>6</b> Hypothesis Tests for the mean of a population</a><ul>
<li class="chapter" data-level="6.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#writing-hypotheses"><i class="fa fa-check"></i><b>6.1</b> Writing Hypotheses</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#null-and-alternative-hypotheses"><i class="fa fa-check"></i><b>6.1.1</b> Null and alternative hypotheses</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#error"><i class="fa fa-check"></i><b>6.1.2</b> Error</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#why-should-hypotheses-use-mu-and-not-barx"><i class="fa fa-check"></i><b>6.1.3</b> Why should hypotheses use <span class="math inline">\(\mu\)</span> and not <span class="math inline">\(\bar{x}\)</span>?</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#calculating-p-values"><i class="fa fa-check"></i><b>6.1.4</b> Calculating p-values</a></li>
<li class="chapter" data-level="6.1.5" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#calculating-p-values-vs-cutoff-values"><i class="fa fa-check"></i><b>6.1.5</b> Calculating p-values vs cutoff values</a></li>
<li class="chapter" data-level="6.1.6" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#t-tests-in-r"><i class="fa fa-check"></i><b>6.1.6</b> t-tests in R</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>6.2</b> Type I and Type II Errors</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#power-and-sample-size-selection"><i class="fa fa-check"></i><b>6.2.1</b> Power and Sample Size Selection</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><i class="fa fa-check"></i><b>7</b> Two-Sample Hypothesis Tests and Confidence Intervals</a><ul>
<li class="chapter" data-level="7.1" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#difference-in-means-between-two-groups"><i class="fa fa-check"></i><b>7.1</b> Difference in means between two groups</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#inference-via-resampling"><i class="fa fa-check"></i><b>7.1.1</b> Inference via resampling</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#inference-via-asymptotic-results-unequal-variance-assumption"><i class="fa fa-check"></i><b>7.1.2</b> Inference via asymptotic results (unequal variance assumption)</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#inference-via-asymptotic-results-equal-variance-assumption"><i class="fa fa-check"></i><b>7.1.3</b> Inference via asymptotic results (equal variance assumption)</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#difference-in-means-between-two-groups-paired-data"><i class="fa fa-check"></i><b>7.2</b> Difference in means between two groups: Paired Data</a></li>
<li class="chapter" data-level="7.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>7.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html"><i class="fa fa-check"></i><b>8</b> Testing Model Assumptions</a><ul>
<li class="chapter" data-level="8.1" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#testing-normality"><i class="fa fa-check"></i><b>8.1</b> Testing Normality</a><ul>
<li class="chapter" data-level="8.1.1" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#visual-inspection---qqplots"><i class="fa fa-check"></i><b>8.1.1</b> Visual Inspection - QQplots</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#tests-for-normality"><i class="fa fa-check"></i><b>8.1.2</b> Tests for Normality</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#testing-equal-variance"><i class="fa fa-check"></i><b>8.2</b> Testing Equal Variance</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#visual-inspection"><i class="fa fa-check"></i><b>8.2.1</b> Visual Inspection</a></li>
<li class="chapter" data-level="8.2.2" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#tests-for-equal-variance"><i class="fa fa-check"></i><b>8.2.2</b> Tests for Equal Variance</a></li>
<li class="chapter" data-level="8.2.3" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#symmetry-of-the-f-distribution"><i class="fa fa-check"></i><b>8.2.3</b> Symmetry of the F-distribution</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#power-of-the-f-test"><i class="fa fa-check"></i><b>8.3</b> Power of the F-test</a></li>
<li class="chapter" data-level="8.4" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#theoretical-distribution-vs-bootstrap"><i class="fa fa-check"></i><b>8.4</b> Theoretical distribution vs bootstrap</a></li>
<li class="chapter" data-level="8.5" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>9</b> Analysis of Variance (ANOVA)</a><ul>
<li class="chapter" data-level="9.1" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#model"><i class="fa fa-check"></i><b>9.1</b> Model</a></li>
<li class="chapter" data-level="9.2" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#theory"><i class="fa fa-check"></i><b>9.2</b> Theory</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#anova-table"><i class="fa fa-check"></i><b>9.2.1</b> Anova Table</a></li>
<li class="chapter" data-level="9.2.2" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#anova-using-simple-vs-complex-models."><i class="fa fa-check"></i><b>9.2.2</b> ANOVA using Simple vs Complex models.</a></li>
<li class="chapter" data-level="9.2.3" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#parameter-estimates-and-confidence-intervals"><i class="fa fa-check"></i><b>9.2.3</b> Parameter Estimates and Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#anova-in-r"><i class="fa fa-check"></i><b>9.3</b> Anova in R</a></li>
<li class="chapter" data-level="9.4" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#multiple-comparisons"><i class="fa fa-check"></i><b>9.4</b> Multiple comparisons</a></li>
<li class="chapter" data-level="9.5" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#different-model-representations"><i class="fa fa-check"></i><b>9.5</b> Different Model Representations</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#theory-1"><i class="fa fa-check"></i><b>9.5.1</b> Theory</a></li>
<li class="chapter" data-level="9.5.2" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#model-representations-in-r"><i class="fa fa-check"></i><b>9.5.2</b> Model Representations in R</a></li>
<li class="chapter" data-level="9.5.3" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#implications-on-the-anova-table"><i class="fa fa-check"></i><b>9.5.3</b> Implications on the ANOVA table</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-regression.html"><a href="10-regression.html"><i class="fa fa-check"></i><b>10</b> Regression</a><ul>
<li class="chapter" data-level="10.1" data-path="10-regression.html"><a href="10-regression.html#pearsons-correlation-coefficient"><i class="fa fa-check"></i><b>10.1</b> Pearson’s Correlation Coefficient</a></li>
<li class="chapter" data-level="10.2" data-path="10-regression.html"><a href="10-regression.html#model-theory"><i class="fa fa-check"></i><b>10.2</b> Model Theory</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-regression.html"><a href="10-regression.html#anova-interpretation"><i class="fa fa-check"></i><b>10.2.1</b> Anova Interpretation</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-regression.html"><a href="10-regression.html#confidence-intervals-vs-prediction-intervals"><i class="fa fa-check"></i><b>10.2.2</b> Confidence Intervals vs Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10-regression.html"><a href="10-regression.html#extrapolation"><i class="fa fa-check"></i><b>10.3</b> Extrapolation</a></li>
<li class="chapter" data-level="10.4" data-path="10-regression.html"><a href="10-regression.html#checking-model-assumptions"><i class="fa fa-check"></i><b>10.4</b> Checking Model Assumptions</a></li>
<li class="chapter" data-level="10.5" data-path="10-regression.html"><a href="10-regression.html#common-problems"><i class="fa fa-check"></i><b>10.5</b> Common Problems</a><ul>
<li class="chapter" data-level="10.5.1" data-path="10-regression.html"><a href="10-regression.html#influential-points"><i class="fa fa-check"></i><b>10.5.1</b> Influential Points</a></li>
<li class="chapter" data-level="10.5.2" data-path="10-regression.html"><a href="10-regression.html#transformations"><i class="fa fa-check"></i><b>10.5.2</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html"><i class="fa fa-check"></i><b>11</b> Resampling Linear Models</a><ul>
<li class="chapter" data-level="11.1" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#using-lm-for-many-analyses"><i class="fa fa-check"></i><b>11.1</b> Using <code>lm()</code> for many analyses</a><ul>
<li class="chapter" data-level="11.1.1" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#one-sample-t-tests"><i class="fa fa-check"></i><b>11.1.1</b> One-sample t-tests</a></li>
<li class="chapter" data-level="11.1.2" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#two-sample-t-tests"><i class="fa fa-check"></i><b>11.1.2</b> Two-sample t-tests</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#creating-simulated-data"><i class="fa fa-check"></i><b>11.2</b> Creating Simulated Data</a><ul>
<li class="chapter" data-level="11.2.1" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#observational-studies-vs-designed-experiments"><i class="fa fa-check"></i><b>11.2.1</b> Observational Studies vs Designed Experiments</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#confidence-interval-types"><i class="fa fa-check"></i><b>11.3</b> Confidence Interval Types</a><ul>
<li class="chapter" data-level="11.3.1" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#normal-intervals"><i class="fa fa-check"></i><b>11.3.1</b> Normal intervals</a></li>
<li class="chapter" data-level="11.3.2" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#percentile-intervals"><i class="fa fa-check"></i><b>11.3.2</b> Percentile intervals</a></li>
<li class="chapter" data-level="11.3.3" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#basic-intervals"><i class="fa fa-check"></i><b>11.3.3</b> Basic intervals</a></li>
<li class="chapter" data-level="11.3.4" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#towards-bias-corrected-and-accelerated-intervals-bca"><i class="fa fa-check"></i><b>11.3.4</b> Towards bias-corrected and accelerated intervals (BCa)</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#bootstrap-confidence-intervals-in-r"><i class="fa fa-check"></i><b>11.4</b> Bootstrap Confidence Intervals in R</a><ul>
<li class="chapter" data-level="11.4.1" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#using-carboot-function"><i class="fa fa-check"></i><b>11.4.1</b> Using <code>car::Boot()</code> function</a></li>
<li class="chapter" data-level="11.4.2" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#using-the-boot-package"><i class="fa fa-check"></i><b>11.4.2</b> Using the <code>boot</code> package</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-contingency-tables.html"><a href="12-contingency-tables.html"><i class="fa fa-check"></i><b>12</b> Contingency Tables</a><ul>
<li class="chapter" data-level="12.1" data-path="12-contingency-tables.html"><a href="12-contingency-tables.html#expected-counts"><i class="fa fa-check"></i><b>12.1</b> Expected Counts</a></li>
<li class="chapter" data-level="12.2" data-path="12-contingency-tables.html"><a href="12-contingency-tables.html#hypothesis-testing"><i class="fa fa-check"></i><b>12.2</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="12.3" data-path="12-contingency-tables.html"><a href="12-contingency-tables.html#rxc-tables"><i class="fa fa-check"></i><b>12.3</b> RxC tables</a></li>
<li class="chapter" data-level="12.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Methodology</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Regression</h1>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="10-regression.html#cb320-1"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb320-2"><a href="10-regression.html#cb320-2"></a><span class="kw">library</span>(dplyr)</span>
<span id="cb320-3"><a href="10-regression.html#cb320-3"></a><span class="kw">library</span>(ggfortify)  <span class="co"># for diagnostic plots in ggplot2 via autoplot()</span></span></code></pre></div>
<p>We continue to want to examine the relationship between a predictor variable and a response but now we consider the case that the predictor is continuous and the response is also continuous. In general we are going to be interested in finding the line that best fits the observed data and determining if we should include the predictor variable in the model.</p>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<div id="pearsons-correlation-coefficient" class="section level2">
<h2><span class="header-section-number">10.1</span> Pearson’s Correlation Coefficient</h2>
<p>We first consider Pearson’s correlation coefficient, which is a statistics that measures the strength of the linear relationship between the predictor and response. Consider the following Pearson’s correlation statistic
<span class="math display">\[r=\frac{\sum_{i=1}^{n}\left(\frac{x_{i}-\bar{x}}{s_{x}}\right)\left(\frac{y_{i}-\bar{y}}{s_{y}}\right)}{n-1}\]</span>
where <span class="math inline">\(x_{i}\)</span> and <span class="math inline">\(y_{i}\)</span> are the x and y coordinate of the <span class="math inline">\(i\)</span>th observation. Notice that each parenthesis value is the standardized value of each observation. If the x-value is big (greater than <span class="math inline">\(\bar{x}\)</span>) and the y-value is large (greater than <span class="math inline">\(\bar{y}\)</span>), then after multiplication, the result is positive. Likewise if the x-value is small and the y-value is small, both standardized values are negative and therefore after multiplication the result is positive. If a large x-value is paired with a small y-value, then the first value is positive, but the second is negative and so the multiplication result is negative.</p>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The following are true about Pearson’s correlation coefficient:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(r\)</span> is unit-less because we have standardized the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values.</li>
<li><span class="math inline">\(-1\le r\le1\)</span> because of the scaling by <span class="math inline">\(n-1\)</span></li>
<li>A negative <span class="math inline">\(r\)</span> denotes a negative relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, while a positive value of <span class="math inline">\(r\)</span> represents a positive relationship.</li>
<li><span class="math inline">\(r\)</span> measures the strength of the linear relationship between the predictor and response.</li>
</ol>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="model-theory" class="section level2">
<h2><span class="header-section-number">10.2</span> Model Theory</h2>
<p>To scatterplot data that looks linear we often want to fit the model
<span class="math display">\[y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\;\;\;\textrm{where }\epsilon_{i}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)\]</span>
where</p>
<table>
<colgroup>
<col width="23%" />
<col width="21%" />
<col width="55%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Parameter</th>
<th align="center">Name</th>
<th align="left">Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\beta_0\)</span></td>
<td align="center">y-intercept</td>
<td align="left">Height of regression line at <span class="math inline">\(x=0\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\beta_1\)</span></td>
<td align="center">slope</td>
<td align="left">How much the line rises for a <span class="math inline">\(1\)</span> unit increase in <span class="math inline">\(x\)</span>.</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\sigma\)</span></td>
<td align="center">Standard Deviation</td>
<td align="left">The “typical” distance from a point to the regression line</td>
</tr>
</tbody>
</table>
<p>The assumptions of this model are:</p>
<ol style="list-style-type: decimal">
<li><em>The relationship between the predictor and response is actually linear.</em></li>
<li><em>The error terms come from a normal distribution.</em></li>
<li><em>The variance of the errors is the same for every value of x (homoscedasticity).</em></li>
<li><em>The error terms are independent.</em></li>
</ol>
<p>Under this model, the expected value of an observation with covariate <span class="math inline">\(X=x\)</span> is <span class="math inline">\(E\left(Y\,|\,X=x\right)=\beta_{0}+\beta_{1}x\)</span> and a new observation has a standard deviation of <span class="math inline">\(\sigma\)</span> about the line.</p>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Given this model, how do we find estimates of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>? In the past we have always relied on using some sort of sample mean, but it is not obvious what we can use here. Instead of a mean, we will use the values of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> that minimize the sum of squared error (SSE) where
<span class="math display">\[\begin{aligned}
\hat{y}_{i}	&amp;=	\hat{\beta}_{0}+\hat{\beta}_{1}x_{i} \\
e_{i}	      &amp;=	y_{i}-\hat{y}_{i} \\
SSE	        &amp;=	\sum_{i=1}^{n}e_{i}^{2}
\end{aligned}\]</span></p>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Fortunately there are simple closed form solutions for <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>
<span class="math display">\[\begin{aligned}
\hat{\beta}_{1}	&amp;=	r\,\left(\frac{s_{y}}{s_{x}}\right)\\
\hat{\beta_{0}}	&amp;=	\bar{y}-\hat{\beta}_{1}\bar{x} 
\end{aligned}\]</span></p>
<p>and using these estimates several properties can be shown</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> are the intercept and slope values that minimize SSE.</li>
<li>The regression line goes through the center of mass of the data (<span class="math inline">\(\bar{x}\)</span>,<span class="math inline">\(\bar{y}\)</span>).</li>
<li>The sum of the residuals is 0. That is: <span class="math inline">\(\sum e_{i}=0\)</span>.</li>
<li><span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> are unbiased estimators of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>.</li>
</ol>
<p>We are also interested in an estimate of <span class="math inline">\(\sigma^{2}\)</span> and we will use our usual estimation scheme of
<span class="math display">\[\begin{aligned} \hat{\sigma}^{2}	
  &amp;= \frac{1}{n-2}\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}  
	=	\frac{\sum_{i=1}^{n}e_{i}^{2}}{n-2} 
	=	\frac{SSE}{n-2} 
	=	MSE 
	\end{aligned}\]</span></p>
<p>where the <span class="math inline">\(-2\)</span> comes from having to estimate <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> before we can estimate <span class="math inline">\(\sigma^{2}\)</span>. As in the ANOVA case, we can interpret <span class="math inline">\(\sigma\)</span> as the typical distance an observation is from its predicted value.</p>
<p>As always we are also interested in knowing the estimated standard deviation (which we will call Standard Error) of the model parameters <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> and it can be shown that
<span class="math display">\[StdErr\left(\hat{\beta}_{0}\right)=\hat{\sigma}\sqrt{\frac{1}{n}+\frac{\bar{x}^{2}}{S_{xx}}}\]</span>
and
<span class="math display">\[StdErr\left(\hat{\beta}_{1}\right)=\hat{\sigma}\sqrt{\frac{1}{S_{xx}}}\]</span>
where <span class="math inline">\(S_{xx}=\sum\left(x_{i}-\bar{x}\right)^{2}\)</span>. These intervals can be used to calculate confidence intervals for <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> using the formulas:
<span class="math display">\[\hat{\beta}_{i}\pm t_{n-2}^{1-\alpha/2}StdErr\left(\hat{\beta}_{i}\right)\]</span></p>
<p>Again we consider the iris dataset that is available in R. I wish to examine the relationship between sepal length and sepal width in the species <em>setosa</em>.</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="10-regression.html#cb321-1"></a>setosa &lt;-<span class="st"> </span>iris <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>( Species <span class="op">==</span><span class="st"> &#39;setosa&#39;</span> )    <span class="co"># Just setosa!</span></span>
<span id="cb321-2"><a href="10-regression.html#cb321-2"></a><span class="kw">ggplot</span>(setosa, <span class="kw">aes</span>(<span class="dt">x=</span>Sepal.Length, <span class="dt">y=</span>Sepal.Width)) <span class="op">+</span></span>
<span id="cb321-3"><a href="10-regression.html#cb321-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb321-4"><a href="10-regression.html#cb321-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&quot;Sepal Length&quot;</span>, <span class="dt">y=</span><span class="st">&quot;Sepal Width&quot;</span>, <span class="dt">title=</span><span class="st">&#39;Setosa Irises&#39;</span>) </span></code></pre></div>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="10-regression.html#cb322-1"></a><span class="co"># Do all the crazy calculations &quot;By Hand!&quot;</span></span>
<span id="cb322-2"><a href="10-regression.html#cb322-2"></a>x &lt;-<span class="st"> </span>setosa<span class="op">$</span>Sepal.Length</span>
<span id="cb322-3"><a href="10-regression.html#cb322-3"></a>y &lt;-<span class="st"> </span>setosa<span class="op">$</span>Sepal.Width</span>
<span id="cb322-4"><a href="10-regression.html#cb322-4"></a>n &lt;-<span class="st"> </span><span class="kw">length</span>(x)</span>
<span id="cb322-5"><a href="10-regression.html#cb322-5"></a>r &lt;-<span class="st"> </span><span class="kw">sum</span>( (x<span class="op">-</span><span class="kw">mean</span>(x))<span class="op">/</span><span class="kw">sd</span>(x) <span class="op">*</span><span class="st"> </span>(y<span class="op">-</span><span class="kw">mean</span>(y))<span class="op">/</span><span class="kw">sd</span>(y) ) <span class="op">/</span><span class="st"> </span>(n<span class="dv">-1</span>)</span>
<span id="cb322-6"><a href="10-regression.html#cb322-6"></a>b1 &lt;-<span class="st"> </span>r<span class="op">*</span><span class="kw">sd</span>(y)<span class="op">/</span><span class="kw">sd</span>(x)</span>
<span id="cb322-7"><a href="10-regression.html#cb322-7"></a>b0 &lt;-<span class="st"> </span><span class="kw">mean</span>(y) <span class="op">-</span><span class="st"> </span>b1<span class="op">*</span><span class="kw">mean</span>(x)</span>
<span id="cb322-8"><a href="10-regression.html#cb322-8"></a><span class="kw">cbind</span>(r, b0, b1)</span></code></pre></div>
<pre><code>##              r         b0        b1
## [1,] 0.7425467 -0.5694327 0.7985283</code></pre>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="10-regression.html#cb324-1"></a>yhat &lt;-<span class="st"> </span>b0 <span class="op">+</span><span class="st"> </span>b1<span class="op">*</span>x</span>
<span id="cb324-2"><a href="10-regression.html#cb324-2"></a>resid &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span>yhat</span>
<span id="cb324-3"><a href="10-regression.html#cb324-3"></a>SSE &lt;-<span class="st"> </span><span class="kw">sum</span>( resid<span class="op">^</span><span class="dv">2</span> )</span>
<span id="cb324-4"><a href="10-regression.html#cb324-4"></a>s2 &lt;-<span class="st"> </span>SSE<span class="op">/</span>(n<span class="dv">-2</span>)</span>
<span id="cb324-5"><a href="10-regression.html#cb324-5"></a>s2</span></code></pre></div>
<pre><code>## [1] 0.06580573</code></pre>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="10-regression.html#cb326-1"></a>Sxx &lt;-<span class="st"> </span><span class="kw">sum</span>( (x<span class="op">-</span><span class="kw">mean</span>(x))<span class="op">^</span><span class="dv">2</span> )</span>
<span id="cb326-2"><a href="10-regression.html#cb326-2"></a>stderr.b0 &lt;-<span class="st"> </span><span class="kw">sqrt</span>(s2) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">1</span><span class="op">/</span>n <span class="op">+</span><span class="st"> </span><span class="kw">mean</span>(x)<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>Sxx)</span>
<span id="cb326-3"><a href="10-regression.html#cb326-3"></a>stderr.b1 &lt;-<span class="st"> </span><span class="kw">sqrt</span>(s2) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>Sxx )</span>
<span id="cb326-4"><a href="10-regression.html#cb326-4"></a><span class="kw">cbind</span>(stderr.b0, stderr.b1)</span></code></pre></div>
<pre><code>##      stderr.b0 stderr.b1
## [1,] 0.5217119 0.1039651</code></pre>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="10-regression.html#cb328-1"></a>t.star &lt;-<span class="st"> </span><span class="kw">qt</span>(.<span class="dv">975</span>, <span class="dt">df=</span>n<span class="dv">-2</span>)	</span>
<span id="cb328-2"><a href="10-regression.html#cb328-2"></a><span class="kw">c</span>(b0<span class="op">-</span>t.star<span class="op">*</span>stderr.b0, b0<span class="op">+</span>t.star<span class="op">*</span>stderr.b0)</span></code></pre></div>
<pre><code>## [1] -1.6184048  0.4795395</code></pre>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="10-regression.html#cb330-1"></a><span class="kw">c</span>(b1<span class="op">-</span>t.star<span class="op">*</span>stderr.b1, b1<span class="op">+</span>t.star<span class="op">*</span>stderr.b1)</span></code></pre></div>
<pre><code>## [1] 0.5894925 1.0075641</code></pre>
<p>Of course, we don’t want to have to do these calculations by hand. Fortunately statistics packages will do all of the above calculations. In R, we will use <code>lm()</code> to fit a linear regression model and then call various accessor functions to give me the regression output I want.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="10-regression.html#cb332-1"></a><span class="kw">cor</span>( setosa<span class="op">$</span>Sepal.Width,  setosa<span class="op">$</span>Sepal.Length )</span></code></pre></div>
<pre><code>## [1] 0.7425467</code></pre>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="10-regression.html#cb334-1"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>(Sepal.Width <span class="op">~</span><span class="st"> </span>Sepal.Length, <span class="dt">data=</span>setosa)</span>
<span id="cb334-2"><a href="10-regression.html#cb334-2"></a><span class="kw">coef</span>(model)</span></code></pre></div>
<pre><code>##  (Intercept) Sepal.Length 
##   -0.5694327    0.7985283</code></pre>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="10-regression.html#cb336-1"></a><span class="kw">confint</span>(model)</span></code></pre></div>
<pre><code>##                   2.5 %    97.5 %
## (Intercept)  -1.6184048 0.4795395
## Sepal.Length  0.5894925 1.0075641</code></pre>
<p>In general, most statistics programs will give a table of output summarizing a regression and the table is usually set up as follows:</p>
<table>
<colgroup>
<col width="11%" />
<col width="12%" />
<col width="16%" />
<col width="36%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Coefficient</th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t-stat</th>
<th align="center">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Intercept |</td>
<td align="center"><span class="math inline">\(\hat{\beta}_{0}\)</span></td>
<td align="center">StdErr<span class="math inline">\((\hat{\beta}_0)\)</span></td>
<td align="center"><span class="math inline">\(t_{0}=\frac{\hat{\beta}_0}{StdErr(\hat{\beta}_0)}\)</span> | $2*</td>
<td align="center"><span class="math inline">\(2*P(T_{n-2}&gt;\vert t_0 \vert )\)</span> |</td>
</tr>
<tr class="even">
<td align="center">Slope | $\h</td>
<td align="center"><span class="math inline">\(\hat{\beta}_{1}\)</span></td>
<td align="center">StdErr<span class="math inline">\((\hat{\beta}_1)\)</span> | $</td>
<td align="center"><span class="math inline">\(t_{1}=\frac{\hat{\beta}_1}{StdErr(\hat{\beta}_1)}\)</span> | $2*</td>
<td align="center"><span class="math inline">\(2*P(T_{n-2}&gt;\vert t_1 \vert )\)</span> |</td>
</tr>
</tbody>
</table>
<p>This table is printed by R by using the <code>summary()</code> function:</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="10-regression.html#cb338-1"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>(Sepal.Width <span class="op">~</span><span class="st"> </span>Sepal.Length, <span class="dt">data=</span>setosa)</span>
<span id="cb338-2"><a href="10-regression.html#cb338-2"></a><span class="kw">summary</span>(model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sepal.Width ~ Sepal.Length, data = setosa)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.72394 -0.18273 -0.00306  0.15738  0.51709 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -0.5694     0.5217  -1.091    0.281    
## Sepal.Length   0.7985     0.1040   7.681 6.71e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2565 on 48 degrees of freedom
## Multiple R-squared:  0.5514,	Adjusted R-squared:  0.542 
## F-statistic: 58.99 on 1 and 48 DF,  p-value: 6.71e-10</code></pre>
<p>The first row is giving information about the y-intercept. In this case the estimate is <span class="math inline">\(-0.5694\)</span> and the standard error of the estimate is <span class="math inline">\(0.5217\)</span>. The t-statistic and associated p-value is testing the hypotheses:
<span class="math inline">\(H_{0}:\,\beta_{0}=0\)</span> vs <span class="math inline">\(H_{a}:\,\beta_{0}\ne0\)</span>. This test is not usually of much interest. However because the equivalent test in the slope row testing <span class="math inline">\(\beta_{1}=0\)</span> vs <span class="math inline">\(\beta_{1}\ne0\)</span>, the p-value of the slope row is <em>very</em> interesting because it tells me if I should include the slope variable in the model. If <span class="math inline">\(\beta_{1}\)</span> could be zero, then we should drop the predictor from our model and use the simple model <span class="math inline">\(y_{i}=\beta_{0}+\epsilon_{i}\)</span> instead.</p>
<p>There are a bunch of other statistics that are returned by <code>summary()</code>. The Residual standard error is just <span class="math inline">\(\hat{\sigma}=\sqrt{MSE}\)</span> and the degrees of freedom for that error is also given. The rest are involved with the ANOVA interpretation of a linear model.</p>
<div id="anova-interpretation" class="section level3">
<h3><span class="header-section-number">10.2.1</span> Anova Interpretation</h3>
<p>Just as in the ANOVA analysis, we really have a competition between two models. The full model <span class="math display">\[y_{i}=\beta_{0}+\beta_{1}x+\epsilon_{i}\]</span>
vs the simple model where x does not help predict <span class="math inline">\(y\)</span>
<span class="math display">\[y_{i}=\beta_0+\epsilon_{i}\]</span>
Notice this is effectively forcing the regression line to be flay and I could have written the model using <span class="math inline">\(\beta_{0}=\mu\)</span> to try to keep our notation straight. If I were to look at the simple model I would use <span class="math inline">\(\bar{y}=\hat{\beta}_0\)</span> as the predicted value of <span class="math inline">\(y\)</span> for <em>any</em> value of <span class="math inline">\(x\)</span> and my Sum of Squared Error in the simple model will be
<span class="math display">\[SSE_{simple}	=	\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}
	=	\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_0\right)^{2}\]</span>
and the appropriate Mean Squared Error is</p>
<p><span class="math display">\[MSE_{simple}=\frac{1}{n-1}\sum\left(y_{i}-\hat{\beta}_0\right)^{2}\]</span></p>
<p>We can go through the same sort of calculations for the full complex model and get
<span class="math display">\[SSE_{complex}	=	\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}
	=	\sum_{i=1}^{n}\left(y_{i}-\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{i}\right)\right)^{2}\]</span>
Notice that <span class="math inline">\(\hat{\beta}_0\)</span> term is in both models, but will not be numerically the same.
Next we have
<span class="math display">\[MSE_{complex}=\frac{1}{n-2}\sum_{i=1}^{n}\left(y_{i}-\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{i}\right)\right)^{2}\]</span>
Just as in the AVOVA analysis, if we often like to look at the difference between
<span class="math display">\[SSE_{simple}-SSE_{comples}=SSE_{diff}\]</span>
and think of this quantity as the amount of variability that is explained by adding the slope parameter to the model. Just as in the AVOVA case we’ll calculate
<span class="math display">\[MSE_{diff}=SSE_{diff}/df_{diff}\]</span> where <span class="math inline">\(df_{diff}\)</span> is the number of parameters that we added to the simple model to create the complex one. In the simple linear regression case, <span class="math inline">\(df_{diff}=1\)</span>.</p>
<p>Just as in the ANOVA case, we will calculate an f-statistic to test the null hypothesis that the simple model suffices vs the alternative that the complex model is necessary. The calculation is <span class="math display">\[f=\frac{MSE_{diff}}{MSE_{complex}}\]</span> and the associated p-value is <span class="math inline">\(P\left(F_{1,n-2}&gt;f\right)\)</span>. Notice that this test is exactly testing if <span class="math inline">\(\beta_{1}=0\)</span> and therefore the p-value for the F-test and the t-test for <span class="math inline">\(\beta_{1}\)</span> are the same. It can easily be shown that <span class="math inline">\(t_{1}^{2}=f\)</span>.</p>
<p>The Analysis of Variance table looks the same as what we have seen, but now we recognize that the rows actually represent the complex and simple models and the difference between them.</p>
<table style="width:100%;">
<colgroup>
<col width="8%" />
<col width="5%" />
<col width="12%" />
<col width="27%" />
<col width="30%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">df</th>
<th align="center">Sum Sq</th>
<th align="center">MS</th>
<th align="center">F</th>
<th align="center">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Difference</td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(SSE_{diff}\)</span></td>
<td align="center"><span class="math inline">\(MSE_{diff} = SSE_{diff}/1\)</span></td>
<td align="center"><span class="math inline">\(f = \frac{MSE_{diff}}{MSE_{complex}}\)</span></td>
<td align="center"><span class="math inline">\(P(F_{1,n-2} &gt; f)\)</span></td>
</tr>
<tr class="even">
<td align="center">Complex</td>
<td align="center"><span class="math inline">\(n-2\)</span></td>
<td align="center"><span class="math inline">\(SSE_{complex}\)</span></td>
<td align="center"><span class="math inline">\(MSE{complex} = SSE_{complex}/(n-2\)</span></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Simple</td>
<td align="center"><span class="math inline">\(n-1\)</span></td>
<td align="center"><span class="math inline">\(SSE_{simple}\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>As usual, the ANOVA table for the regression is available in R using the <code>anova()</code> command.</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="10-regression.html#cb340-1"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>(Sepal.Width <span class="op">~</span><span class="st"> </span>Sepal.Length, <span class="dt">data=</span>setosa)</span>
<span id="cb340-2"><a href="10-regression.html#cb340-2"></a><span class="kw">anova</span>(model)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Sepal.Width
##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Sepal.Length  1 3.8821  3.8821  58.994 6.71e-10 ***
## Residuals    48 3.1587  0.0658                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>But we notice that R chooses not to display the row corresponding to the simple model.</p>
<p>I could consider <span class="math inline">\(SSE_{simple}\)</span> as a baseline measure of the amount of variability in the data. It is interesting to look at how much of that baseline variability has been explained by adding the additional parameter to the model. Therefore we’ll define the ratio <span class="math inline">\(R^{2}\)</span> as: <span class="math display">\[R^{2}=\frac{SSE_{diff}}{SSE_{simple}}=\frac{SSE_{simple}-SSE_{complex}}{SSE_{simple}}=r^{2}\]</span> where <span class="math inline">\(r\)</span> is Pearson’s Correlation Coefficient. <span class="math inline">\(R^{2}\)</span> has the wonderful interpretation of the percent of variability in the response variable that can be explained by the predictor variable <span class="math inline">\(x\)</span>.</p>
</div>
<div id="confidence-intervals-vs-prediction-intervals" class="section level3">
<h3><span class="header-section-number">10.2.2</span> Confidence Intervals vs Prediction Intervals</h3>
<p>There are two different types of questions that we might ask about predicting the value for some x-value <span class="math inline">\(x_{new}\)</span>.</p>
<p>We might be interested in a confidence interval for regression line. For this question we want to know how much would we expect the sample regression line move if we were to collect a new set of data. In particular, for some value of <span class="math inline">\(x\)</span>, say <span class="math inline">\(x_{new}\)</span>, how variable would the regression line be? To answer that we have to ask what is the estimated variance of <span class="math inline">\(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\)</span>? The variance of the regression line will be a function of the variances of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> and thus the standard error looks somewhat reminiscent of the standard errors of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>. Recalling that we defined <span class="math inline">\(S_{xx}=\sum\left(x_{i}-\bar{x}\right)^{2}\)</span>, we have:
<span class="math display">\[\hat{Var}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\right)=\hat{\sigma}^{2}\left(\frac{1}{n}+\frac{\left(x_{new}-\bar{x}\right)^{2}}{S_{xx}}\right)\]</span>
and therefore its <span class="math inline">\(StdErr(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new})\)</span> is
<span class="math display">\[StdErr\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\right)=\hat{\sigma}\sqrt{\frac{1}{n}+\frac{\left(x_{new}-\bar{x}\right)^{2}}{S_{xx}}}\]</span></p>
<p>We can use this value to produce a confidence interval for the regression line for any value of <span class="math inline">\(x_{new}\)</span>.
<span class="math display">\[Estimate	\pm	t\;StdErr\left(Estimate\right)\]</span>
<span class="math display">\[\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\right)	\pm	t_{n-2}^{1-\alpha/2}\;\;\hat{\sigma}\sqrt{\frac{1}{n}+\frac{\left(x_{new}-\bar{x}\right)^{2}}{S_{xx}}}\]</span></p>
<p>the expected value of new observation <span class="math inline">\(\hat{E}\left(Y\,|\,X=x_{new}\right)\)</span>. This expectation is regression line but because the estimated regression line is a function of the data, then the line isn’t the exactly the same as the true regression line. To reflect that, I want to calculate a confidence interval for where the true regression line should be.</p>
<p>I might instead be interested calculating a confidence interval for <span class="math inline">\(y_{new}\)</span>, which I will call a <em>prediction</em> interval in an attempt to keep from being confused with the confidence interval of the regression line. Because we have <span class="math display">\[y_{new}=\beta_{0}+\beta_{1}x_{new}+\epsilon_{new}\]</span></p>
<p>then my prediction interval will still be centered at <span class="math inline">\(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\)</span> but the the uncertainty should be the sum of the uncertainty associated with the estimates of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> and the additional variability associated with <span class="math inline">\(\epsilon_{new}\)</span>. In short,
<span class="math display">\[\begin{aligned}
\hat{Var}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}+\epsilon\right)	
    &amp;=	\hat{Var}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\right)+\hat{Var}\left(\epsilon\right) \\
	  &amp;=	\hat{\sigma}^{2}\left(\frac{1}{n}+\frac{\left(x_{new}-\bar{x}\right)^{2}}{S_{xx}}\right)+\hat{\sigma}^{2}
	  \end{aligned}\]</span></p>
<p>and the <span class="math inline">\(StdErr\left(\right)\)</span> of a new observation will be</p>
<p><span class="math display">\[StdErr\left(\hat{y}_{new}\right)=\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{\left(x_{new}-\bar{x}\right)^{2}}{S_{xx}}}\]</span></p>
<p>So the prediction interval for a new observation will be:
<span class="math display">\[\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\right)\pm t_{n-2}^{1-\alpha/2}\;\;\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{\left(x_{new}-\bar{x}\right)^{2}}{S_{xx}}}\]</span></p>
<p>To emphasize the difference between confidence regions (capturing where we believe the regression line to lay) versus prediction regions (where new data observations will lay) we note that as the sample size increases, the uncertainty as to where the regression line lays decreases, but the prediction intervals will always contain a minimum width due to the error associated with an individual observation. Below are confidence (red) and prediction (blue) regions for two different sample sizes.</p>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>In general, you will not want to calculate the confidence intervals and prediction intervals by hand. Fortunately R makes it easy to calculate the intervals. The function <code>predict()</code> will calculate the point estimates along with confidence and prediction intervals. The function requires the <code>lm()</code> output along with an optional data frame (if you want to predict values not in the original data).</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="10-regression.html#cb342-1"></a><span class="kw">ggplot</span>(setosa, <span class="kw">aes</span>(<span class="dt">x=</span>Sepal.Length, <span class="dt">y=</span>Sepal.Width)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb342-2"><a href="10-regression.html#cb342-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb342-3"><a href="10-regression.html#cb342-3"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Sepal Length vs Sepal Width&#39;</span>)</span></code></pre></div>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="10-regression.html#cb343-1"></a><span class="co">#fit the regression</span></span>
<span id="cb343-2"><a href="10-regression.html#cb343-2"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>(Sepal.Width <span class="op">~</span><span class="st"> </span>Sepal.Length, <span class="dt">data=</span>setosa)</span>
<span id="cb343-3"><a href="10-regression.html#cb343-3"></a></span>
<span id="cb343-4"><a href="10-regression.html#cb343-4"></a><span class="co"># display the first few predictions</span></span>
<span id="cb343-5"><a href="10-regression.html#cb343-5"></a><span class="kw">head</span>( <span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>) )</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 3.503062 3.427519 3.578604
## 2 3.343356 3.267122 3.419590
## 3 3.183650 3.086634 3.280666
## 4 3.103798 2.991890 3.215705
## 5 3.423209 3.350256 3.496162
## 6 3.742620 3.632603 3.852637</code></pre>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="10-regression.html#cb345-1"></a><span class="co"># predict at x = 5.0</span></span>
<span id="cb345-2"><a href="10-regression.html#cb345-2"></a><span class="kw">predict</span>(model, </span>
<span id="cb345-3"><a href="10-regression.html#cb345-3"></a>        <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>,                  <span class="co"># prediction Interval</span></span>
<span id="cb345-4"><a href="10-regression.html#cb345-4"></a>        <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">Sepal.Length =</span> <span class="fl">5.0</span>)) <span class="co"># at x=5</span></span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 3.423209 2.902294 3.944123</code></pre>
<p>We can create a nice graph of the regression line and associated confidence and prediction regions using the following code in R:</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="10-regression.html#cb347-1"></a><span class="co"># ask for the confidence and prediction intervals</span></span>
<span id="cb347-2"><a href="10-regression.html#cb347-2"></a>conf.region &lt;-<span class="st"> </span><span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&#39;confidence&#39;</span>)</span>
<span id="cb347-3"><a href="10-regression.html#cb347-3"></a>pred.region &lt;-<span class="st"> </span><span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&#39;prediction&#39;</span>)</span>
<span id="cb347-4"><a href="10-regression.html#cb347-4"></a></span>
<span id="cb347-5"><a href="10-regression.html#cb347-5"></a><span class="co"># add them to my original data frame</span></span>
<span id="cb347-6"><a href="10-regression.html#cb347-6"></a>setosa &lt;-<span class="st"> </span>setosa <span class="op">%&gt;%</span></span>
<span id="cb347-7"><a href="10-regression.html#cb347-7"></a><span class="st">  </span><span class="kw">mutate</span>( <span class="dt">fit =</span> <span class="kw">fitted</span>(model),</span>
<span id="cb347-8"><a href="10-regression.html#cb347-8"></a>          <span class="dt">conf.lwr =</span> conf.region[,<span class="dv">2</span>],</span>
<span id="cb347-9"><a href="10-regression.html#cb347-9"></a>          <span class="dt">conf.upr =</span> conf.region[,<span class="dv">3</span>],</span>
<span id="cb347-10"><a href="10-regression.html#cb347-10"></a>          <span class="dt">pred.lwr =</span> pred.region[,<span class="dv">2</span>],</span>
<span id="cb347-11"><a href="10-regression.html#cb347-11"></a>          <span class="dt">pred.upr =</span> pred.region[,<span class="dv">3</span>])</span></code></pre></div>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="10-regression.html#cb348-1"></a><span class="co"># make a nice plot</span></span>
<span id="cb348-2"><a href="10-regression.html#cb348-2"></a><span class="kw">ggplot</span>(setosa) <span class="op">+</span></span>
<span id="cb348-3"><a href="10-regression.html#cb348-3"></a><span class="st">  </span><span class="kw">geom_point</span>(  <span class="kw">aes</span>(<span class="dt">x=</span>Sepal.Length, <span class="dt">y=</span>Sepal.Width) ) <span class="op">+</span></span>
<span id="cb348-4"><a href="10-regression.html#cb348-4"></a><span class="st">  </span><span class="kw">geom_line</span>(   <span class="kw">aes</span>(<span class="dt">x=</span>Sepal.Length, <span class="dt">y=</span>fit), <span class="dt">col=</span><span class="st">&#39;red&#39;</span> ) <span class="op">+</span></span>
<span id="cb348-5"><a href="10-regression.html#cb348-5"></a><span class="st">  </span><span class="kw">geom_ribbon</span>( <span class="kw">aes</span>(<span class="dt">x=</span>Sepal.Length, <span class="dt">ymin=</span>conf.lwr, <span class="dt">ymax=</span>conf.upr), <span class="dt">fill=</span><span class="st">&#39;red&#39;</span>,  <span class="dt">alpha=</span>.<span class="dv">4</span>) <span class="op">+</span></span>
<span id="cb348-6"><a href="10-regression.html#cb348-6"></a><span class="st">  </span><span class="kw">geom_ribbon</span>( <span class="kw">aes</span>(<span class="dt">x=</span>Sepal.Length, <span class="dt">ymin=</span>pred.lwr, <span class="dt">ymax=</span>pred.upr), <span class="dt">fill=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">4</span>)</span></code></pre></div>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>It is worth noting that these confidence intervals are all point-wise confidence intervals. If I want to calculate confidence or prediction intervals for a large number of <span class="math inline">\(x_{new}\)</span> values, then I have to deal with the multiple comparisons issue. Fortunately this is easy to do in the simple linear regression case. Instead of using the <span class="math inline">\(t_{n-2}^{1-\alpha/2}\)</span> quantile in the interval formulas, we should use <span class="math inline">\(W=\sqrt{2*F_{1-\alpha,\,2,\,n-2}}\)</span>. Many books ignore this issue as does the <code>predict()</code> function in R.</p>
</div>
</div>
<div id="extrapolation" class="section level2">
<h2><span class="header-section-number">10.3</span> Extrapolation</h2>
<p>The data observed will inform a researcher about the relationship between the x and y variables, but only in the range for which you have data! Below are the winning times of the men’s 1500 meter Olympic race.</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="10-regression.html#cb349-1"></a><span class="kw">data</span>(men1500m, <span class="dt">package=</span><span class="st">&#39;HSAUR2&#39;</span>)</span>
<span id="cb349-2"><a href="10-regression.html#cb349-2"></a>small &lt;-<span class="st"> </span>men1500m <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>( year <span class="op">!=</span><span class="st"> </span><span class="dv">1896</span> )  <span class="co"># Remove the 1896 Olympics</span></span>
<span id="cb349-3"><a href="10-regression.html#cb349-3"></a></span>
<span id="cb349-4"><a href="10-regression.html#cb349-4"></a><span class="co"># fit the model and get the prediction interval</span></span>
<span id="cb349-5"><a href="10-regression.html#cb349-5"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>( time <span class="op">~</span><span class="st"> </span>year, <span class="dt">data=</span>small )</span>
<span id="cb349-6"><a href="10-regression.html#cb349-6"></a>small &lt;-<span class="st"> </span><span class="kw">cbind</span>(small, <span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&#39;prediction&#39;</span>) )</span>
<span id="cb349-7"><a href="10-regression.html#cb349-7"></a></span>
<span id="cb349-8"><a href="10-regression.html#cb349-8"></a><span class="kw">ggplot</span>(small, <span class="kw">aes</span>(<span class="dt">x=</span>year, <span class="dt">y=</span>time, <span class="dt">ymin=</span>lwr, <span class="dt">ymax=</span>upr)) <span class="op">+</span></span>
<span id="cb349-9"><a href="10-regression.html#cb349-9"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb349-10"><a href="10-regression.html#cb349-10"></a><span class="st">  </span><span class="kw">geom_line</span>( <span class="kw">aes</span>(<span class="dt">y=</span>fit), <span class="dt">col=</span><span class="st">&#39;red&#39;</span> ) <span class="op">+</span></span>
<span id="cb349-11"><a href="10-regression.html#cb349-11"></a><span class="st">  </span><span class="kw">geom_ribbon</span>( <span class="dt">fill=</span><span class="st">&#39;light blue&#39;</span>,  <span class="dt">alpha=</span>.<span class="dv">4</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb349-12"><a href="10-regression.html#cb349-12"></a><span class="st">  </span><span class="kw">labs</span>( <span class="dt">x=</span><span class="st">&#39;Year&#39;</span>, <span class="dt">y=</span><span class="st">&#39;Time (s)&#39;</span>, <span class="dt">title=</span><span class="st">&#39;Winning times of Mens 1500 m&#39;</span> ) <span class="op">+</span><span class="st"> </span></span>
<span id="cb349-13"><a href="10-regression.html#cb349-13"></a><span class="st">  </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>If we are interested in predicting the results of the 2008 and 2012 Olympic race, what would we predict?</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="10-regression.html#cb350-1"></a><span class="kw">predict</span>(model, </span>
<span id="cb350-2"><a href="10-regression.html#cb350-2"></a>        <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">year=</span><span class="kw">c</span>(<span class="dv">2008</span>, <span class="dv">2012</span>)), </span>
<span id="cb350-3"><a href="10-regression.html#cb350-3"></a>        <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 208.1293 199.3971 216.8614
## 2 206.8451 198.0450 215.6453</code></pre>
<p>We can compare the predicted intervals with the time actually recorded by the winner of the men’s 1500m. In Beijing 2008, Rashid Ramzi from Brunei won the event in 212.94 seconds and in London 2012 Taoufik Makhloufi from Algeria won in 214.08 seconds. Both times are within the corresponding prediction intervals, but clearly the linear relationship must eventually change and therefore our regression could not possibly predict the winning time of the 3112 race.</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="10-regression.html#cb352-1"></a><span class="kw">predict</span>(model, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">year=</span><span class="kw">c</span>(<span class="dv">3112</span>)), <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>)</span></code></pre></div>
<pre><code>##         fit       lwr       upr
## 1 -146.2973 -206.7705 -85.82402</code></pre>
</div>
<div id="checking-model-assumptions" class="section level2">
<h2><span class="header-section-number">10.4</span> Checking Model Assumptions</h2>
<p>As in the ANOVA analysis, we want to be able to check the model assumptions. To do this, we will examine the residuals <span class="math inline">\(e_{i}=y_{i}-\hat{y}_{i}\)</span> for normality using a QQ-plot as we did in ANOVA. To address the constant variance and linearity assumptions we will look at scatterplots of the residuals vs the fitted values <span class="math inline">\(\hat{y}_{i}\)</span>. For the regression to be valid, we want the scatterplot to show no discernible trend. There are two patterns that commonly show up that indicate a violation of the regression assumptions.</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="10-regression.html#cb354-1"></a><span class="kw">set.seed</span>(<span class="dv">2233</span>);</span>
<span id="cb354-2"><a href="10-regression.html#cb354-2"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>));</span>
<span id="cb354-3"><a href="10-regression.html#cb354-3"></a>n &lt;-<span class="st"> </span><span class="dv">20</span>;</span>
<span id="cb354-4"><a href="10-regression.html#cb354-4"></a>x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dt">length=</span>n);</span>
<span id="cb354-5"><a href="10-regression.html#cb354-5"></a>data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb354-6"><a href="10-regression.html#cb354-6"></a>  <span class="dt">Fitted=</span><span class="kw">c</span>(x,x,x),</span>
<span id="cb354-7"><a href="10-regression.html#cb354-7"></a>  <span class="dt">Residual=</span><span class="kw">c</span>(<span class="kw">rnorm</span>(n,<span class="dv">0</span>,.<span class="dv">25</span>), <span class="kw">rnorm</span>(n,(<span class="dv">2</span><span class="op">*</span>x<span class="dv">-1</span>)<span class="op">^</span><span class="dv">2</span><span class="fl">-.375</span>, <span class="fl">.2</span>), <span class="kw">rnorm</span>(n,<span class="dv">0</span>,x<span class="op">*</span>.<span class="dv">45</span>)), </span>
<span id="cb354-8"><a href="10-regression.html#cb354-8"></a>  <span class="dt">Type=</span><span class="kw">factor</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">each=</span>n), <span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&#39;No Trend&#39;</span>, <span class="st">&#39;Non-Linear&#39;</span>, <span class="st">&#39;Non-Constant Variance&#39;</span>) ));</span>
<span id="cb354-9"><a href="10-regression.html#cb354-9"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>){</span>
<span id="cb354-10"><a href="10-regression.html#cb354-10"></a>  index &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>n <span class="op">+</span><span class="st"> </span>n<span class="op">*</span>(i<span class="dv">-1</span>);</span>
<span id="cb354-11"><a href="10-regression.html#cb354-11"></a>  <span class="kw">plot</span>(data<span class="op">$</span>Fitted[index], data<span class="op">$</span>Residual[index], </span>
<span id="cb354-12"><a href="10-regression.html#cb354-12"></a>	   <span class="dt">xlab=</span><span class="st">&#39;Fitted&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Residual&#39;</span>, <span class="dt">main=</span>data<span class="op">$</span>Type[index[<span class="dv">1</span>]] );</span>
<span id="cb354-13"><a href="10-regression.html#cb354-13"></a>  <span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">2</span>);</span>
<span id="cb354-14"><a href="10-regression.html#cb354-14"></a>}</span></code></pre></div>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>To illustrate this, we’ll consider the cherry tree dataset that comes with R. The goal will be predicting the volume of lumber produced by a cherry tree of a given diameter. The data are given in a dataset pre-loaded in R called <code>trees</code>.</p>
<p>Step one: Graph the data. The first step in a regression analysis is to graph the data and think about if a linear relationship makes sense.</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="10-regression.html#cb355-1"></a><span class="kw">head</span>(trees)  <span class="co"># 3 columns Girth, Height, Volume</span></span></code></pre></div>
<pre><code>##   Girth Height Volume
## 1   8.3     70   10.3
## 2   8.6     65   10.3
## 3   8.8     63   10.2
## 4  10.5     72   16.4
## 5  10.7     81   18.8
## 6  10.8     83   19.7</code></pre>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="10-regression.html#cb357-1"></a><span class="kw">ggplot</span>(trees, <span class="kw">aes</span>(<span class="dt">x=</span>Girth, <span class="dt">y=</span>Volume)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb357-2"><a href="10-regression.html#cb357-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb357-3"><a href="10-regression.html#cb357-3"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Volume vs Girth&#39;</span>)</span></code></pre></div>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Initially, it looks like a line is a pretty good description of this relationship.</p>
<p>Step two: Fit a regression and examine the diagnostic plots.</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="10-regression.html#cb358-1"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>( Volume <span class="op">~</span><span class="st"> </span>Girth, <span class="dt">data=</span>trees )</span>
<span id="cb358-2"><a href="10-regression.html#cb358-2"></a><span class="kw">autoplot</span>(model, <span class="dt">which=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span></code></pre></div>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>The normality assumption isn’t too bad, but there is a strong trend in the residual plot. The curvature we see in the residual group is present in the original scatterplot, but it is more obvious. At this point I would think about a slightly more complicated model, e.g. should we include height in the model or perhaps <code>Girth^2</code>? The implications of both of these possibilities will be explored in STA 571 but for now we’ll just continue using the model we have.</p>
<p>Step three: Plot the data and the regression model.</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="10-regression.html#cb359-1"></a>trees &lt;-<span class="st"> </span><span class="kw">cbind</span>( trees, <span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&#39;confidence&#39;</span>) )</span>
<span id="cb359-2"><a href="10-regression.html#cb359-2"></a><span class="kw">head</span>(trees)  <span class="co"># now we have the fit, lwr, upr columns</span></span></code></pre></div>
<pre><code>##   Girth Height Volume       fit       lwr       upr
## 1   8.3     70   10.3  5.103149  2.152294  8.054004
## 2   8.6     65   10.3  6.622906  3.799685  9.446127
## 3   8.8     63   10.2  7.636077  4.896577 10.375578
## 4  10.5     72   16.4 16.248033 14.156839 18.339228
## 5  10.7     81   18.8 17.261205 15.235884 19.286525
## 6  10.8     83   19.7 17.767790 15.774297 19.761284</code></pre>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="10-regression.html#cb361-1"></a><span class="kw">ggplot</span>(trees, <span class="kw">aes</span>(<span class="dt">x=</span>Girth)) <span class="op">+</span></span>
<span id="cb361-2"><a href="10-regression.html#cb361-2"></a><span class="st">  </span><span class="kw">geom_ribbon</span>( <span class="kw">aes</span>( <span class="dt">ymin=</span>lwr, <span class="dt">ymax=</span>upr), <span class="dt">alpha=</span>.<span class="dv">4</span>, <span class="dt">fill=</span><span class="st">&#39;pink&#39;</span> ) <span class="op">+</span></span>
<span id="cb361-3"><a href="10-regression.html#cb361-3"></a><span class="st">  </span><span class="kw">geom_line</span>( <span class="kw">aes</span>(<span class="dt">y=</span>fit), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></span>
<span id="cb361-4"><a href="10-regression.html#cb361-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y=</span>Volume)) </span></code></pre></div>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>In this graph we see that we underestimate the volume for small girths, overestimate for medium values, and underestimate for large girths. So we see the same pattern of the residuals in this graph as we saw in the residual graph. While the model we’ve selected isn’t as good as it could be, this isn’t horribly bad and might suffice for a first pass</p>
<blockquote>
<p>“All models are wrong, but some are useful.” George Box.</p>
</blockquote>
<p>Step four: Evaluate the model coefficients.</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="10-regression.html#cb362-1"></a><span class="kw">summary</span>(model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Volume ~ Girth, data = trees)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -8.065 -3.107  0.152  3.495  9.587 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***
## Girth         5.0659     0.2474   20.48  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.252 on 29 degrees of freedom
## Multiple R-squared:  0.9353,	Adjusted R-squared:  0.9331 
## F-statistic: 419.4 on 1 and 29 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="10-regression.html#cb364-1"></a><span class="kw">confint</span>(model)</span></code></pre></div>
<pre><code>##                  2.5 %     97.5 %
## (Intercept) -43.825953 -30.060965
## Girth         4.559914   5.571799</code></pre>
<p>From the summary output, we can see several things:</p>
<ol style="list-style-type: decimal">
<li><p>The intercept term <span class="math inline">\(\hat{\beta}_{0}\)</span> is significantly different than zero. While we should expect that a tree with zero girth should have zero volume, our model predicts a volume of -36.9, which is obviously ridiculous. I’m not too worried about this because we have no data from trees that small and the intercept is quite the extrapolation from the range of Girth values we actually have. This is primarily being driven by the real relationship having curvature and our model has no curvature in it. So long as we don’t use this model to predict values too far away from our data points, I’m happy.</p></li>
<li><p>The slope is statistically significantly positive. We see an estimate an increase of 5 units of Volume for every 1 unit increase in Girth.</p></li>
<li><p>The estimate <span class="math inline">\(\hat{\sigma}\)</span> is given by the residual standard error and is 4.252 and that is interpreted as the typical distance away from the regression line.</p></li>
<li><p>The R-sq value gives the amount of variability in the data that is explained by the regression line as <span class="math inline">\(93.5\%\)</span>. So the variable Girth explains a huge amount of the variability in volume of lumber a tree produces.</p></li>
<li><p>Finally, the F-test is comparing the complex vs the simple model, which in this case, reduces to just testing if the slope term, <span class="math inline">\(\beta_{1}\)</span>, could be zero. In simple regression, the F-statistic is the square of the t-statistic for testing the slope. That is, F-statistic = <span class="math inline">\(419.4 = 20.48^{2}\)</span>. The p-values are the same for the two tests because they are testing exactly the same hypothesis.</p></li>
</ol>
</div>
<div id="common-problems" class="section level2">
<h2><span class="header-section-number">10.5</span> Common Problems</h2>
<div id="influential-points" class="section level3">
<h3><span class="header-section-number">10.5.1</span> Influential Points</h3>
<p>Sometimes a dataset will contain one observation that has a large effect on the outcome of the model. Consider the following datasets where the red denotes a highly influential point and the red line is the regression line including the point.</p>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>The question of what to do with influential points is not easy to answer. Sometimes these are data points that are a result of lab technician error and should be removed. Sometimes they are the result of an important process that is not well understood by the researcher. It is up to the scientist to figure out which is the case and take appropriate action.</p>
<p>One solution is to run the analysis both with and without the influential point and see how much it affects your inferences.</p>
</div>
<div id="transformations" class="section level3">
<h3><span class="header-section-number">10.5.2</span> Transformations</h3>
<p>When the normality or constant variance assumption is violated, sometimes it is possible to transform the data to make it satisfy the assumption. Often times count data is analyzed as log(count) and weights are analyzed after taking a square root or cube root transform.</p>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>We have the option of either transforming the x-variable or transforming the y-variable or possibly both. One thing to keep in mind, however, is that transforming the x-variable only effects the linearity of the relationship. Transforming the y-variable effects both the linearity and the variance.</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="10-regression.html#cb366-1"></a><span class="kw">set.seed</span>(<span class="op">-</span><span class="dv">838</span>)</span>
<span id="cb366-2"><a href="10-regression.html#cb366-2"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb366-3"><a href="10-regression.html#cb366-3"></a>n &lt;-<span class="st"> </span><span class="dv">40</span></span>
<span id="cb366-4"><a href="10-regression.html#cb366-4"></a>x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">30</span>, <span class="dt">length=</span>n);</span>
<span id="cb366-5"><a href="10-regression.html#cb366-5"></a>y &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">30</span><span class="op">*</span><span class="kw">exp</span>((<span class="dv">30</span><span class="op">-</span>x)<span class="op">/</span><span class="dv">10</span>) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">sd=</span><span class="dv">20</span>)</span>
<span id="cb366-6"><a href="10-regression.html#cb366-6"></a>y &lt;-<span class="st"> </span><span class="kw">abs</span>(y)</span>
<span id="cb366-7"><a href="10-regression.html#cb366-7"></a><span class="kw">plot</span>(x,y); <span class="kw">abline</span>(<span class="kw">coef</span>(<span class="kw">lm</span>(y<span class="op">~</span>x)));</span>
<span id="cb366-8"><a href="10-regression.html#cb366-8"></a><span class="kw">plot</span>(x, <span class="kw">log</span>(y)); <span class="kw">abline</span>(<span class="kw">coef</span>(<span class="kw">lm</span>(<span class="kw">I</span>(<span class="kw">log</span>(y))<span class="op">~</span>x)));</span>
<span id="cb366-9"><a href="10-regression.html#cb366-9"></a><span class="kw">plot</span>(x<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>), y); <span class="kw">abline</span>(<span class="kw">coef</span>(<span class="kw">lm</span>(y<span class="op">~</span><span class="kw">I</span>(x<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)))));</span></code></pre></div>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="10-regression.html#cb367-1"></a>mydata &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)</span></code></pre></div>
<p>Unfortunately it is not always obvious what transformation is most appropriate. The Box-Cox family of transformations for the y-variable is <span class="math display">\[ f(y\,|\,\lambda)	=	\begin{cases}
    y^{\lambda} &amp; \;\;\textrm{if}\,\,\lambda\ne0\\
    \log y &amp; \;\;\textrm{if}\,\,\lambda=0
  \end{cases}\]</span>
which includes squaring (<span class="math inline">\(\lambda=2\)</span>), square root (<span class="math inline">\(\lambda=1/2\)</span>) and as <span class="math inline">\(\lambda \to 0\)</span> the transformation converges to <span class="math inline">\(\log y\)</span>. (To do this correctly we should define the transformation in a more complicated fashion, but that level of detail is unnecessary here.) The transformation is selected by looking at the profile log-likelihood value of different values of <span class="math inline">\(\lambda\)</span> and we want to use the <span class="math inline">\(\lambda\)</span> that maximizes the log-likelihood.</p>
<p>Of course, we also want to use a transformation that isn’t completely obscure and is commonly used in the scientific field, so square roots, reciprocals, and logs are preferred.</p>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="10-regression.html#cb368-1"></a><span class="kw">str</span>(mydata)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:	40 obs. of  2 variables:
##  $ x: num  0 0.769 1.538 2.308 3.077 ...
##  $ y: num  2 3.08 2.92 4.17 5.44 ...</code></pre>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="10-regression.html#cb370-1"></a>MASS<span class="op">::</span><span class="kw">boxcox</span>(y<span class="op">~</span>x, <span class="dt">data=</span>mydata, <span class="dt">plotit=</span><span class="ot">TRUE</span>)</span></code></pre></div>
<p><img src="10_Regression_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>Here we see the resulting confidence interval for <span class="math inline">\(\lambda\)</span> contains 0, so a <span class="math inline">\(\log\)</span> transformation would be most appropriate.</p>
<p>Unfortunately there isn’t a matching procedure for deciding how to transform the <span class="math inline">\(x\)</span> covariate. Usually we spend a great deal of time trying different transformations and see how they affect the scatterplot and using transformations that are common in whatever field the researcher is working in.</p>
<p>In general, deciding on a transformation to use is often a trade-off between statistical pragmatism and interpretability. In cases that a transformation is not possible, or the interpretation is difficult, it is necessary to build more complicated models that are hopefully interpretable. We will explore these issues in great length in STA 571.</p>
</div>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">10.6</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>Use the following data below to answer the questions below</p>
<table>
<tbody>
<tr class="odd">
<td align="center"><strong>x</strong></td>
<td align="center">3</td>
<td align="center">8</td>
<td align="center">10</td>
<td align="center">18</td>
<td align="center">23</td>
<td align="center">28</td>
</tr>
<tr class="even">
<td align="center"><strong>y</strong></td>
<td align="center">14</td>
<td align="center">28</td>
<td align="center">43</td>
<td align="center">62</td>
<td align="center">79</td>
<td align="center">86</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li><p>Plot the data in a scatter plot. <em>The following code might be useful:</em></p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="10-regression.html#cb371-1"></a><span class="co"># read in the data</span></span>
<span id="cb371-2"><a href="10-regression.html#cb371-2"></a>p1.data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb371-3"><a href="10-regression.html#cb371-3"></a>  <span class="dt">x =</span> <span class="kw">c</span>( <span class="dv">3</span>,  <span class="dv">8</span>, <span class="dv">10</span>, <span class="dv">18</span>, <span class="dv">23</span>, <span class="dv">28</span>),</span>
<span id="cb371-4"><a href="10-regression.html#cb371-4"></a>  <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">14</span>, <span class="dv">28</span>, <span class="dv">43</span>, <span class="dv">62</span>, <span class="dv">79</span>, <span class="dv">86</span>)  )</span>
<span id="cb371-5"><a href="10-regression.html#cb371-5"></a></span>
<span id="cb371-6"><a href="10-regression.html#cb371-6"></a><span class="co"># make a nice graph</span></span>
<span id="cb371-7"><a href="10-regression.html#cb371-7"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb371-8"><a href="10-regression.html#cb371-8"></a><span class="kw">ggplot</span>(p1.data, <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)) <span class="op">+</span></span>
<span id="cb371-9"><a href="10-regression.html#cb371-9"></a><span class="st">  </span><span class="kw">geom_point</span>()</span></code></pre></div></li>
<li><p>We will first calculate the regression coefficients and their estimated standard deviations by hand (mostly).</p>
<ol style="list-style-type: lower-roman">
<li>Use R to confirm that that the following summary statistics are correct:</li>
</ol>
<table>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\bar{x}=15\)</span></td>
<td align="center"><span class="math inline">\(s_x=9.59\)</span></td>
<td align="center"><span class="math inline">\(S_{xx}=460\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\bar{y}=52\)</span></td>
<td align="center"><span class="math inline">\(s_y=28.59\)</span></td>
<td align="center"><span class="math inline">\(r = 0.9898\)</span></td>
</tr>
</tbody>
</table>
<ol start="2" style="list-style-type: lower-roman">
<li><p>Using the above statistics, by hand calculate the estimates <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>.</p></li>
<li><p>For each data point, by hand calculate the predicted value <span class="math inline">\(\hat{y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{i}\)</span>.</p></li>
<li><p>For each data point, by hand calculate the estimated error term <span class="math inline">\(\hat{\epsilon}_{i}=y_{i}-\hat{y}_{i}\)</span>.</p></li>
<li><p>Calculate the MSE for the complex model. Using the MSE, what is <span class="math inline">\(\hat{\sigma}\)</span>?</p></li>
<li><p>By hand, calculate the estimated standard deviation (which is often called the standard error) of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>.</p></li>
</ol></li>
<li><p>Use the R function <code>lm()</code> to fit a regression to these data.</p>
<ol style="list-style-type: lower-roman">
<li><p>Using the <code>predict()</code> function, confirm your hand calculation of the <span class="math inline">\(\hat{y}_{i}\)</span> values.</p></li>
<li><p>Using the <code>resid()</code> function, confirm your hand calculation of the <span class="math inline">\(\hat{\epsilon}_{i}\)</span> terms.</p></li>
<li><p>Using the <code>summary()</code> function, confirm your hand calculations of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> and their standard errors.</p></li>
</ol></li>
<li><p>Again using R’s built in functions, give a 95% confidence interval for <span class="math inline">\(\beta_{1}\)</span>.</p></li>
<li><p>Using the appropriate R output, test the hypothesis <span class="math inline">\(H_{0}:\;\beta_{1}=0\)</span> versus the alternative <span class="math inline">\(H_{a}:\;\beta_{1} \ne 0\)</span>.</p></li>
<li><p>Give the R^{2} value for this regression.</p></li>
<li><p>What is the typical distance to the regression line?</p></li>
<li><p>Create a nice graph of the regression line and the confidence interval for the true relationship using the following code:</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="10-regression.html#cb372-1"></a><span class="co"># make a nice graph</span></span>
<span id="cb372-2"><a href="10-regression.html#cb372-2"></a><span class="kw">ggplot</span>(p1.data, <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)) <span class="op">+</span></span>
<span id="cb372-3"><a href="10-regression.html#cb372-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb372-4"><a href="10-regression.html#cb372-4"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&#39;lm&#39;</span>)</span></code></pre></div>
<p>Often I want to create the confidence region myself (perhaps to use a prediction interval instead of a confidence interval), and we could use the following code:</p>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="10-regression.html#cb373-1"></a><span class="kw">library</span>(dplyr)</span>
<span id="cb373-2"><a href="10-regression.html#cb373-2"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>( y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data=</span>p1.data )</span>
<span id="cb373-3"><a href="10-regression.html#cb373-3"></a></span>
<span id="cb373-4"><a href="10-regression.html#cb373-4"></a>p1.data &lt;-<span class="st"> </span>p1.data <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb373-5"><a href="10-regression.html#cb373-5"></a><span class="st">  </span><span class="kw">mutate</span>( <span class="dt">yhat =</span> <span class="kw">predict</span>(model),</span>
<span id="cb373-6"><a href="10-regression.html#cb373-6"></a>          <span class="dt">lwr  =</span> <span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&#39;confidence&#39;</span>)[,<span class="dv">2</span>],</span>
<span id="cb373-7"><a href="10-regression.html#cb373-7"></a>          <span class="dt">upr  =</span> <span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&#39;confidence&#39;</span>)[,<span class="dv">3</span>]  )</span>
<span id="cb373-8"><a href="10-regression.html#cb373-8"></a></span>
<span id="cb373-9"><a href="10-regression.html#cb373-9"></a><span class="co"># make a nice graph</span></span>
<span id="cb373-10"><a href="10-regression.html#cb373-10"></a><span class="kw">ggplot</span>(p1.data, <span class="kw">aes</span>(<span class="dt">x=</span>x)) <span class="op">+</span></span>
<span id="cb373-11"><a href="10-regression.html#cb373-11"></a><span class="st">  </span><span class="kw">geom_ribbon</span>( <span class="kw">aes</span>(<span class="dt">ymin=</span>lwr, <span class="dt">ymax=</span>upr), <span class="dt">fill=</span><span class="st">&#39;pink&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">2</span> ) <span class="op">+</span></span>
<span id="cb373-12"><a href="10-regression.html#cb373-12"></a><span class="st">  </span><span class="kw">geom_line</span>(   <span class="kw">aes</span>(   <span class="dt">y=</span>yhat), <span class="dt">color=</span><span class="st">&#39;green&#39;</span> ) <span class="op">+</span></span>
<span id="cb373-13"><a href="10-regression.html#cb373-13"></a><span class="st">  </span><span class="kw">geom_point</span>(  <span class="kw">aes</span>(   <span class="dt">y=</span>y   ), <span class="dt">color=</span><span class="st">&#39;black&#39;</span> )</span></code></pre></div></li>
</ol></li>
<li><p>Olympic track and field records are broken practically every Olympics. The following is output comparing the gold medal winning performance in the men’s long jump (in inches) versus the years 00 to 84. (In this data set, the year 00 represents 1900, and 84 represents 1984. This is a pre Y2K dataset.) There were <span class="math inline">\(n=19\)</span> Olympic games in that period.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Fill in the blanks in the following summary and anova tables:</p>
<p>Summary:</p>
<table>
<thead>
<tr class="header">
<th align="center">Coefficients</th>
<th align="center">Estimate | Std</th>
<th align="center">Std Error | t-</th>
<th align="center">t-value</th>
<th align="center"><span class="math inline">\(Pr(&gt;|t|)\)</span> |</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">(Intercept)</td>
<td align="center">283.45 | 4.</td>
<td align="center">4.28 |</td>
<td align="center"></td>
<td align="center">&lt; 2e-16 |</td>
</tr>
<tr class="even">
<td align="center">Year</td>
<td align="center">0.613</td>
<td align="center">0.0841</td>
<td align="center">7.289 | 1.27</td>
<td align="center">1.27e-06</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="61%" />
<col width="38%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Residual Standard Error = <span class="math inline">\(\;\;\;\;\;\;\;\;\;\;\)</span> |</td>
<td align="center">R-sq = <span class="math inline">\(\;\;\;\;\;\;\;\;\;\;\)</span> |</td>
</tr>
</tbody>
</table>
<p>Analysis of Variance:</p>
<table>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">df | S</th>
<th align="center">Sum Sq</th>
<th align="center">Mean Sq |</th>
<th align="center">F-value</th>
<th align="center">Pr(&gt;F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Year |</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Residuals |</td>
<td align="center">|</td>
<td align="center"></td>
<td align="center">95.19</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Total</td>
<td align="center">18</td>
<td align="center">6673.2 |</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table></li>
</ol></li>
<li><p>Ott &amp; Longnecker 11.45&amp;47 - In the preliminary studies of a new drug, a pharmaceutical firm needs to obtain information on the relationship between the dose level and potency of the drug. In order to obtain this information, a total of 18 test tubes are inoculated with a virus culture and incubated for an appropriate period of time. Three test tubes are randomly assigned to each of 6 different dose levels. The 18 test tubes are then injected with the randomly assigned dose level of the drug. the measured response is the protective strength of the drug against the virus culture. Due to a problem with a few of the test tubes, only 2 responses were obtained for dose levels 4,8, and 16. The data are:</p>
<table style="width:100%;">
<colgroup>
<col width="14%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Dose</td>
<td align="center">2 | 2 |</td>
<td align="center">2</td>
<td align="center">2 | 4 |</td>
<td align="center">4</td>
<td align="center">4</td>
<td align="center">8</td>
<td align="center">8 | 16 | 16 | 1</td>
<td align="center">16</td>
<td align="center">16 | 16 |</td>
<td align="center">16</td>
<td align="center">32 | 32 |</td>
<td align="center">32</td>
<td align="center">64</td>
<td align="center">64 | 64</td>
<td align="center">64</td>
</tr>
<tr class="even">
<td align="center">Response</td>
<td align="center">5 | 7 |</td>
<td align="center">7</td>
<td align="center">3 | 10 |</td>
<td align="center">10</td>
<td align="center">14 | 15 |</td>
<td align="center">15</td>
<td align="center">17 | 20 |</td>
<td align="center">20</td>
<td align="center">21 | 19 |</td>
<td align="center">19</td>
<td align="center">23 | 29 |</td>
<td align="center">29</td>
<td align="center">28 | 31 |</td>
<td align="center">31</td>
<td align="center">30</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li>We will first fit a regression model to the raw data.
<ol style="list-style-type: lower-roman">
<li>Plot the data and comment on the relationship between the covariate and response.</li>
<li>Fit a linear regression model to these data using the lm() function.</li>
<li>Examine the plot of the residuals vs fitted values. Does there appear to be a problem? Explain.</li>
</ol></li>
<li>Often in drug evaluations, a logarithmic transformation of the dose level will yield a linear relationship between the response variable and the independent variable. Let <span class="math inline">\(x_{i}=\log\left(dose_{i}\right)\)</span> (where log is the natural log). Notice that because the constant variance assumption seems to be met, I don’t wish to transform <span class="math inline">\(y\)</span>.
<ol style="list-style-type: lower-roman">
<li>Plot the response of the drug vs the natural log of the dose levels. Does it appear that a linear model is appropriate?</li>
<li>Fit the linear regression model to these data.</li>
<li>From a plot of the residuals vs the fitted values, does the linear model seem appropriate?</li>
<li>Examine the QQplot of the residuals vs the theoretical normal quantiles. Does the normality assumption appear to be violated? Also perform a Shapiro-Wilks test on the residuals to test of a statistically significant difference from normality. Comment on these results.</li>
<li>What is change in the response variable for every one unit change in log(dose)?</li>
<li>Give a <span class="math inline">\(95\%\)</span> confidence interval for the y-intercept and slope parameters. Is the log(dose) level a statistically significant predictor of the response?</li>
</ol></li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="9-analysis-of-variance-anova.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="11-resampling-linear-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"],
"google": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/570_I/raw/master/10_Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["Statistical-Methods-I.pdf", "PDF"], ["Statistical-Methods-I.epub", "EPUB"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
